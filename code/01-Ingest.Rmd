---
title: "Ingest"
author: "JJayes"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

## Purpose

Scrape the website of the patent data in order to download the patent information and the PDFs.

First I have to get the list of URLs from which to download the pdfs.

So I start with the base URL

```{r}
library(tidyverse)
library(rvest)

url <- "https://tc.prv.se/spd/search?content=elektricitet&lang=en&tab=1&hits=true"

url_2 <- "https://tc.prv.se/spd/search?hits=true&tab=1&sort=filingdate&lang=en&content=elektricitet&start=0&range=50"

html <- read_html(url_2)

# this gets the URLs for each patent page. I'll have to visit each page, get the information about the filling - and then download the PDF.
html %>% 
  html_nodes("#hitlist td a") %>%
  html_attr("href")

```

Let's write a function to do this!

We will need a list of the pages from which to get the URLs.

There are 6009 results. That is 121 pages of results. So I must change the start to increment by 50 each time to 6000.

```{r}
base_url <- "https://tc.prv.se/spd/search?hits=true&tab=1&sort=filingdate&lang=en&content=elektricitet&start=0&range=50"

tbl_of_pages <- str_c("https://tc.prv.se/spd/search?hits=true&tab=1&sort=filingdate&lang=en&content=elektricitet&start=",
                       seq(0, 6000, by = 50),
                       "&range=50") %>% 
  as_tibble() %>% 
  rename(search_url = value)

tbl_of_pages
```

Now I have tibble of pages with 121 URLs.

```{r}
tbl_of_pages %>% 
  slice_sample(n = 1) %>% 
  pull()
```

This seems to work - now need to check if I can get the URLs of each patent from the search pages.

```{r}
url_3 <- "https://tc.prv.se/spd/search?hits=true&tab=1&sort=filingdate&lang=en&content=elektricitet&start=5700&range=50"

html <- read_html(url_3)

html %>% 
  html_nodes("#hitlist td a") %>%
  html_attr("href")
```

Works! Though it seems this may not be necessary as the URLs have a standard structure.
```{r}
"/spd/patent?p2=gHQ4LQJ1rM0&hits=true&tab=1&sort=filingdate&lang=en&content=elektricitet&range=50&hitsstart=5700&start=5749"
```

They all have "&hitsstart=5700&start=5749" at the end so I can just make the list of links methodically.

```{r}
html <- read_html("https://tc.prv.se/spd/patent?p2=HIdBuuVipjw&hits=true&tab=1&sort=filingdate&lang=en&content=elektricitet&range=50&hitsstart=5700&start=5704")


```


### Ingest data from patents.google.com

[Google Patents](https://patents.google.com/?q=electricity&country=SE&sort=old) is a searchable online repository of patents.

I've searched for patents registered in Sweden using the term "electricity" or "elektricitet" and downloaded the links. There are 12,000.

```{r}
library(tidyverse)
library(here)
df <- readxl::read_excel(here("data", "patent-info-google", "Electricity_patents_list_Google_Patents.xlsx"), skip = 1)
```

Cleaning and feature engineering.

```{r}
df <- df %>% 
  mutate(across(contains("date"), as.Date)) %>% 
  janitor::clean_names()

# df %>% write_rds(here("data", "patent-info-google", "Electricity_patents_list_Google_Patents.rds"))
```

```{r}
df %>% 
  mutate(year = lubridate::year(priority_date)) %>% 
  count(year) %>% 
  ggplot(aes(year, n)) +
  geom_col(fill = "midnightblue") +
  scale_y_continuous(labels = scales::number_format()) +
  labs(y = "Patents including the term 'Electricity'",
       x = NULL)
```

### Scrape contents for text?

```{r}
df %>% 
  slice(12L) %>% 
  select(result_link) %>% 
  pull()
```


```{r}
library(rvest)

url <- "https://patents.google.com/patent/SE127438C1/en"

html <- read_html(url)
```

#### Tables

Need to get table titles to select the right ones

```{r}
html %>% 
  html_nodes("h2,table") %>% 
  html_text() %>% 
  as_tibble()
  html_table()

html %>%
  html_nodes("table") %>%
  html_nodes("thead") %>%
  html_text() %>%
  as_tibble() %>%
  summarise(n_items = str_count(value, "\\\n")) %>% 
  mutate(rn = row_number())


  
  
  html_table() %>% map_dfr(., bind_rows) %>% view
  map_df(tibble(
    titles = html_nodes("thead") %>% html_text()
    tables = html_nodes("table") %>% nest(html_table())
  ))
  html_nodes("th")
  html_text() %>% 
  as_tibble()
  html_table()
```

```{r}
html %>% 
  html_table()
```

What do I want? The ones with 5 columns and 3 columns

```{r}
html %>% 
  html_table() %>% 
  filter()
```

### Countries

```{r}
html %>%
    html_nodes("[itemprop='countryStatus']") %>%
    html_nodes("[itemprop='countryCode']") %>%
    html_text()

```

### Other citations

```{r}

# href
html %>%
  html_nodes("[itemprop='backwardReferencesFamily']") %>%
  html_nodes("a") %>% 
  html_attr("href")

# patent numbers
html %>%
  html_nodes("[itemprop='backwardReferencesFamily']") %>%
  html_nodes("[itemprop='publicationNumber']") %>% 
  html_text()

# primaryLanguage
html %>%
  html_nodes("[itemprop='backwardReferencesFamily']") %>%
  html_nodes("[itemprop='primaryLanguage']") %>% 
  html_text()

# publicationDate
html %>%
  html_nodes("[itemprop='backwardReferencesFamily']") %>%
  html_nodes("[itemprop='publicationDate']") %>% 
  html_text() %>% 
  lubridate::ymd()

# title
html %>%
  html_nodes("[itemprop='backwardReferencesFamily']") %>%
  html_nodes("[itemprop='title']") %>% 
  html_text()

# title
html %>%
  html_nodes("[itemprop='backwardReferencesFamily']") %>%
  html_nodes("[itemprop='title']") %>% 
  html_text()

```

# Forward links

```{r}
# forwardReferences
# url
html %>%
  html_nodes("[itemprop='forwardReferences']") %>%
  html_nodes("a") %>% 
  html_attr("href")

# publicationNumber
html %>%
  html_nodes("[itemprop='forwardReferences']") %>%
  html_nodes("[itemprop='publicationNumber']") %>% 
  html_text()

# primaryLanguage
html %>%
  html_nodes("[itemprop='forwardReferences']") %>%
  html_nodes("[itemprop='primaryLanguage']") %>% 
  html_text()

# publicationDate
html %>%
  html_nodes("[itemprop='forwardReferences']") %>%
  html_nodes("[itemprop='publicationDate']") %>% 
  html_text() %>% 
  lubridate::ymd()

# assigneeOriginal
html %>%
  html_nodes("[itemprop='forwardReferences']") %>%
  html_nodes("[itemprop='assigneeOriginal']") %>% 
  html_text()

# title
html %>%
  html_nodes("[itemprop='forwardReferences']") %>%
  html_nodes("[itemprop='title']") %>% 
  html_text()

```

Also published as

```{r}
# docdbFamily
# url
html %>%
  html_nodes("[itemprop='docdbFamily']") %>%
  html_nodes("a") %>% 
  html_attr("href")

# publicationNumber
html %>%
  html_nodes("[itemprop='docdbFamily']") %>%
  html_nodes("[itemprop='publicationNumber']") %>% 
  html_text()

# primaryLanguage
html %>%
  html_nodes("[itemprop='docdbFamily']") %>%
  html_nodes("[itemprop='primaryLanguage']") %>% 
  html_text()

# publicationDate
html %>%
  html_nodes("[itemprop='docdbFamily']") %>%
  html_nodes("[itemprop='publicationDate']") %>% 
  html_text() %>% 
  lubridate::ymd()

```


Similar Documents

```{r}
# url
html %>%
  html_nodes("[itemprop='similarDocuments']") %>%
  html_nodes("a") %>% 
  html_attr("href")

# isPatent
html %>%
  html_nodes("[itemprop='similarDocuments']") %>%
  html_nodes("[itemprop='isPatent']") %>% 
  html_attr("content") 

# publicationNumber
html %>%
  html_nodes("[itemprop='similarDocuments']") %>%
  html_nodes("[itemprop='publicationNumber']") %>% 
  html_text()

# primaryLanguage
html %>%
  html_nodes("[itemprop='similarDocuments']") %>%
  html_nodes("[itemprop='primaryLanguage']") %>% 
  html_text()

# title
html %>%
  html_nodes("[itemprop='similarDocuments']") %>%
  html_nodes("[itemprop='title']") %>% 
  html_text(trim = T)


```


## Refactoring

```{r}

get_all_citation_items <- function() {
  items <- tibble(key = c("publicationNumber", "primaryLanguage"))

  items %>%
    mutate(value = possibly(~ get_citation_item(part), "failed"))
}

get_citation_item <- function(part) {
  html %>%
    html_nodes("[itemprop='backwardReferencesFamily']") %>%
    html_nodes(0("[itemprop='", part, "']")) %>%
    html_text()
}

test_function <- function(url){
  
  html <- read_html(url)
  
  html %>% 
    get_all_citation_items
  
}

test_function(url)

```


## Title

```{r}
html %>% 
  html_nodes("title") %>% 
  html_text(trim = T) %>% 
  str_squish() %>% 
  str_remove(., " - Google Patents")
```


```{r}
html %>% 
  html_nodes(".media-body") %>% 
  map_df(~{
    data_frame(
      postal = html_node(.x, "span") %>% html_text(trim=TRUE),
      city = html_nodes(.x, "ul > li") %>% html_text(trim=TRUE)
    )
  }) 
```


#### Text including description and claims

It's written in Swedish and then in English - perhaps I can try to 

```{r}
html %>% 
  html_nodes(".description-paragraph") %>% 
  html_nodes(".google-src-text") %>% 
  html_text()


html %>% 
  html_nodes(".description-paragraph") %>% 
  html_nodes(".notranslate") %>% 
  # html_nodes("span") %>% 
  html_text(trim = T) %>% as_tibble()


html %>% 
  html_nodes(".description-paragraph") %>% 
  html_nodes(".notranslate")


html %>% 
  html_nodes(".google-src-text") %>% 
  html_nodes(".style-scope") %>% 
  html_nodes(".patent-text")



  html_text(trim = T) %>% 
  as_tibble() %>% 
  rename(description = value) %>%
  mutate(n_chat = nchar(description),
         swedish = str_locate_all(description, "\\.")) %>% view
```

```{r}
html %>% 
  html_nodes(".claim-text") %>% 
  html_text(trim = T) %>% 
  as_tibble() %>% 
  rename(description = value) %>% slice(3L) %>% pull()
  mutate(description = str_squish(description))
```


Praise [this Stackoverflow post](https://stackoverflow.com/questions/57627835/rvest-scraping-html-content-values) for helping me find how to get each item.

```{r}
html %>% 
  html_nodes("[itemprop='concept']") %>% 
  html_nodes("[itemprop='name']") %>% 
  html_text()
    
```




```{r}
url <- "https://www.pole-emploi.fr/annuaire/votre-pole-emploi/provins-77160"
html <- read_html(url)

html %>% 
  html_nodes(".media-body") %>% 
  map_df(~{
    data_frame(
      postal = html_node(.x, "span") %>% html_text(trim=TRUE),
      city = html_nodes(.x, "ul > li") %>% html_text(trim=TRUE)
    )
  }) 
```

### Function to iterate through patent information

I want to take out the name, the count.

```{r}
get_patent_info <- function(page_html) {
  
  title <- html %>%
    html_nodes("title") %>%
    html_text(trim = T) %>%
    str_squish() %>%
    str_remove(., " - Google Patents")

  ids <- html %>%
    html_nodes("[itemprop='concept']") %>%
    html_nodes("[itemprop='id']") %>%
    html_text()

  names <- html %>%
    html_nodes("[itemprop='concept']") %>%
    html_nodes("[itemprop='name']") %>%
    html_text()

  counts <- html %>%
    html_nodes("[itemprop='concept']") %>%
    html_nodes("[itemprop='count']") %>%
    html_text()

  concepts <- tibble(ids, names, counts) %>%
    nest(concepts = c(ids, names, counts))
  
  tibble(title, concepts)
}


```

To refactor here:

```{r}
# html %>%
#     html_nodes("[itemprop='concept']") %>%
#     html_nodes("[itemprop='count']") %>%
#     html_text()
#  itemprops <- c("id", "name", "count")
# get_itemprop(html, item){
#   
#   html %>%
#     html_nodes("[itemprop='concept']") %>%
#     html_nodes(paste0("[itemprop='", item, "']") %>%
#     html_text()
#   
# }

```


# Whole function:

```{r}
get_patent_info <- function(url) {
  
  html <- read_html(url)
  
  title <- html %>%
    html_nodes("title") %>%
    html_text(trim = T) %>%
    str_squish() %>%
    str_remove(., " - Google Patents")
  
  ids <- html %>%
    html_nodes("[itemprop='concept']") %>%
    html_nodes("[itemprop='id']") %>%
    html_text()

  names <- html %>%
    html_nodes("[itemprop='concept']") %>%
    html_nodes("[itemprop='name']") %>%
    html_text()

  counts <- html %>%
    html_nodes("[itemprop='concept']") %>%
    html_nodes("[itemprop='count']") %>%
    html_text()

  concepts <- tibble(ids, names, counts) %>%
    nest(concepts = c(ids, names, counts))
  
  tibble(title, concepts)
}
```


```{r}
df %>% 
  as_tibble() %>% 
  janitor::clean_names() %>% 
  mutate(filing_date = lubridate::ymd(filing_date),
         year = lubridate::year(filing_date)) %>%
  count(year) %>% 
  ggplot(aes(year, n)) +
  geom_col(fill = "darkred")

```


